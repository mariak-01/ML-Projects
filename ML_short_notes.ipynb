{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. linear regression**"
      ],
      "metadata": {
        "id": "58r7Nrlnd4jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Take input data (x) and try to predict output (y).\n",
        "\n",
        "- Draw a straight line (y = w·x + b) that best fits the data.\n",
        "\n",
        "- Calculate error (difference between predicted and actual y).\n",
        "\n",
        "- Use cost function (like MSE) to measure how wrong the line is.\n",
        "\n",
        "- Use gradient descent to adjust the line (w and b) to reduce error.\n",
        "\n",
        "- Repeat until the error is very low and the line fits well."
      ],
      "metadata": {
        "id": "K1tvVvpLdvV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. logistic regression**"
      ],
      "metadata": {
        "id": "t4YoNJMkd_ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Take input data (x), multiply by weight, add bias:\n",
        "→ z = w·x + b\n",
        "\n",
        "- Apply sigmoid function to get probability between 0 and 1:\n",
        "→ y = 1 / (1 + e^(-z))\n",
        "\n",
        "- If y ≥ 0.5, predict class 1; else, predict class 0.\n",
        "\n",
        "- Compare prediction with actual label to calculate loss (Log Loss).\n",
        "\n",
        "- Use gradient descent to update weights and reduce error.\n",
        "\n",
        "- Repeat until model makes good predictions."
      ],
      "metadata": {
        "id": "qXmXvVqbeaoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. decision tree**"
      ],
      "metadata": {
        "id": "1xpNDrquejp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Start with full dataset.\n",
        "\n",
        "- Choose the best feature to split the data (based on Gini or Entropy).\n",
        "\n",
        "- Split data into branches based on feature values.\n",
        "\n",
        "- Repeat the process for each branch (subsets of data).\n",
        "\n",
        "- Stop when:\n",
        "\n",
        "    - All data in a node belong to the same class, or\n",
        "\n",
        "    - Maximum depth is reached.\n",
        "\n",
        "- Make prediction by following the path from root to leaf based on input features."
      ],
      "metadata": {
        "id": "8sRuIjNFevW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. random forest**"
      ],
      "metadata": {
        "id": "M94S_dhtgF3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create many decision trees, not just one.\n",
        "\n",
        "- For each tree:\n",
        "\n",
        "    - Use random subset of data (sampling with replacement → bagging).\n",
        "\n",
        "    - Use random subset of features for splitting at each node.\n",
        "\n",
        "- Each tree makes a prediction.\n",
        "\n",
        "- Final output:\n",
        "\n",
        "    - Classification → Majority vote from all trees.\n",
        "\n",
        "    - Regression → Average of all tree outputs."
      ],
      "metadata": {
        "id": "fbWHiB6Bfoj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. knn**"
      ],
      "metadata": {
        "id": "YX1eQsDmgg3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Store all the training data (no training happens initially).\n",
        "\n",
        "- When a new input comes:\n",
        "\n",
        "    - Calculate distance between input and all training points (e.g., Euclidean distance).\n",
        "\n",
        "- Pick K closest neighbors (smallest distances).\n",
        "\n",
        "- Vote among those neighbors:\n",
        "\n",
        "    - Classification → Pick the most common class (majority vote).\n",
        "\n",
        "    - Regression → Take the average of their values."
      ],
      "metadata": {
        "id": "dyzWwdsegMmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. svm**"
      ],
      "metadata": {
        "id": "wiyrvwyXg4BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot the data points of different classes.\n",
        "\n",
        "- Find the best boundary (line or hyperplane) that separates the classes.\n",
        "\n",
        "- This boundary should have the maximum margin — the farthest distance from the closest points (called support vectors).\n",
        "\n",
        "- For non-linearly separable data:\n",
        "\n",
        "    - Use kernel trick to transform data into a higher dimension where it becomes separable."
      ],
      "metadata": {
        "id": "Ah7XcZKPg56j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. naive bayes**"
      ],
      "metadata": {
        "id": "THGR3weih2Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Based on Bayes’ Theorem:\n",
        "𝑃 ( 𝐶 𝑙 𝑎 𝑠 𝑠 ∣ 𝐷 𝑎 𝑡 𝑎 ) = 𝑃 ( 𝐷 𝑎 𝑡 𝑎 ∣ 𝐶 𝑙 𝑎 𝑠 𝑠 ) × 𝑃 ( 𝐶 𝑙 𝑎 𝑠 𝑠 ) 𝑃 ( 𝐷 𝑎 𝑡 𝑎 )\n",
        "\n",
        "\n",
        "- It calculates the probability of each class given the input data.\n",
        "\n",
        "- Assumes that features are independent (that’s the “naive” part).\n",
        "\n",
        "- For each class:\n",
        "\n",
        "    - Multiply prior probability (how common the class is) with\n",
        "\n",
        "    - Likelihood (how likely data features are in that class).\n",
        "\n",
        "- Pick the class with the highest probability."
      ],
      "metadata": {
        "id": "4TpVYffBhGh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. k-Means Clustering**"
      ],
      "metadata": {
        "id": "c3voorhe5K4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Choose a number of clusters (K).\n",
        "\n",
        "- Randomly place K centroids.\n",
        "\n",
        "- Assign each point to the nearest centroid.\n",
        "\n",
        "- Move centroids to the center of their assigned points.\n",
        "\n",
        "- Repeat until centroids don’t move much.\n",
        "\n",
        "- Use to find hidden groups in data."
      ],
      "metadata": {
        "id": "HDi3DRKY5JDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. PCA (Principal Component Analysis) – for dimensionality reduction**"
      ],
      "metadata": {
        "id": "FN6l8v-O5asH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Unsupervised technique (no labels used).\n",
        "\n",
        "- Finds directions (principal components) where data varies the most.\n",
        "\n",
        "- Projects data into fewer dimensions while keeping most information.\n",
        "\n",
        "- Often used before applying other ML models."
      ],
      "metadata": {
        "id": "HG_NSqnJiOWA"
      }
    }
  ]
}